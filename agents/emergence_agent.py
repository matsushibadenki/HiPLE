# path: ./agents/emergence_agent.py
# title: Emergence Agent for Creative Brainstorming
# description: Orchestrates a multi-expert brainstorming session to generate novel ideas.

from typing import List, Dict, Any, Optional
from domain.schemas import ExpertModel
from agents.base_agent import BaseAgent
from services.model_loader import ModelLoaderService
from llama_cpp.llama_types import ChatCompletionRequestMessage

class EmergenceAgent(BaseAgent):
    """
    è¤‡æ•°ã®å°‚é–€å®¶ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ˆã‚‹è­°è«–ã‚’ä¿ƒé€²ã—ã€
    å˜ä¸€ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã¯åˆ°é”ã§ããªã„å‰µç™ºçš„ãªã‚¢ã‚¤ãƒ‡ã‚¢ã‚„è§£æ±ºç­–ã‚’ç”Ÿã¿å‡ºã™ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚
    """
    def __init__(self, model_loader: ModelLoaderService):
        super().__init__(model_loader)
        self.discussion_rounds = 2 # è­°è«–ã®ã‚¿ãƒ¼ãƒ³æ•°

    def execute(self, prompt: str, all_experts: List[ExpertModel]) -> Dict[str, Any]:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã¤ã„ã¦ã€ãƒ–ãƒ¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒŸãƒ³ã‚°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
        """
        print(f"âœ¨ å‰µç™ºã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é–‹å§‹ã—ã¾ã™: {prompt}")

        # 1. è­°è«–ã«å‚åŠ ã™ã‚‹å¤šæ§˜ãªã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸å‡º
        participants = self._select_participants(all_experts)
        if len(participants) < 2:
            return {
                "response": "å‰µç™ºã‚»ãƒƒã‚·ãƒ§ãƒ³ã«å¿…è¦ãªã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚",
                "self_evaluation": {"confidence": 0.1, "reasoning": "Could not find enough diverse experts to hold a discussion."}
            }
        
        print(f"ğŸ‘¥ å‚åŠ è€…: {[p.name for p in participants]}")

        # 2. ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ã§ã®è­°è«–ã‚’å®Ÿè¡Œ
        discussion_history = f"# è­°é¡Œ: {prompt}\n\n"
        for i in range(self.discussion_rounds):
            print(f"ğŸ”„ è­°è«–ãƒ©ã‚¦ãƒ³ãƒ‰ {i+1}/{self.discussion_rounds}")
            for expert in participants:
                print(f"   ğŸ—£ï¸ {expert.name} ã®ã‚¿ãƒ¼ãƒ³...")
                contribution_prompt = self._create_contribution_prompt(prompt, discussion_history, expert)
                messages: List[ChatCompletionRequestMessage] = [
                    {"role": "system", "content": expert.system_prompt},
                    {"role": "user", "content": contribution_prompt}
                ]
                
                response_data = self._query_llm(expert, messages)
                contribution = response_data.get("response", f"ï¼ˆ{expert.name}ã¯å¿œç­”ã—ã¾ã›ã‚“ã§ã—ãŸï¼‰")

                discussion_history += f"## {expert.name} (å°‚é–€: {expert.description}) ã®æ„è¦‹:\n{contribution}\n\n---\n"

        # 3. æœ€çµ‚çš„ãªçµ±åˆå½¹ãŒè­°è«–ã‚’è¦ç´„ã—ã€å‰µç™ºçš„ãªçµè«–ã‚’å°ãå‡ºã™
        synthesizer = self._find_expert("HRM", all_experts)
        if not synthesizer:
             synthesizer = participants[0] # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        
        print(f"âœ… è­°è«–ãŒçµ‚äº†ã—ã¾ã—ãŸã€‚çµ±åˆå½¹ ({synthesizer.name}) ãŒçµè«–ã‚’ã¾ã¨ã‚ã¾ã™ã€‚")
        synthesis_prompt = self._create_synthesis_prompt(prompt, discussion_history)
        messages_synth: List[ChatCompletionRequestMessage] = [
            {"role": "system", "content": "ã‚ãªãŸã¯ã€è¤‡æ•°ã®å°‚é–€å®¶ã«ã‚ˆã‚‹æ´»ç™ºãªè­°è«–ã‚’åˆ†æã—ã€ãã“ã‹ã‚‰æœ€ã‚‚é‡è¦ã§é©æ–°çš„ãªæ´å¯Ÿã‚’æŠ½å‡ºã—ã€ä¸€ã¤ã®é¦–å°¾ä¸€è²«ã—ãŸçµè«–ã«çµ±åˆã™ã‚‹ã€æ¥µã‚ã¦å„ªç§€ãªãƒ•ã‚¡ã‚·ãƒªãƒ†ãƒ¼ã‚¿ãƒ¼å…¼ã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼ã§ã™ã€‚"},
            {"role": "user", "content": synthesis_prompt}
        ]
        
        final_result_data = self._query_llm(synthesizer, messages_synth)
        print("âœ¨ å‰µç™ºã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        
        return final_result_data

    def _select_participants(self, all_experts: List[ExpertModel]) -> List[ExpertModel]:
        """è­°è«–ã®ãŸã‚ã«å¤šæ§˜ãªã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠã™ã‚‹"""
        selected_experts = []
        # æ€è€ƒ(HRM)ã€æ±ç”¨(Jamba)ã€ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°(Transformer)ã‚’å„ªå…ˆçš„ã«é¸å‡º
        participant_names = ["hrm", "jamba", "transformer"]
        for name in participant_names:
            expert = self._find_expert(name, all_experts)
            if expert:
                selected_experts.append(expert)
        return selected_experts

    def _create_contribution_prompt(self, original_prompt: str, history: str, current_expert: ExpertModel) -> str:
        """å„ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã«æ„è¦‹ã‚’æ±‚ã‚ã‚‹ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã™ã‚‹"""
        return f"""ç¾åœ¨ã€ä»¥ä¸‹ã®è­°é¡Œã«ã¤ã„ã¦ãƒ–ãƒ¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒŸãƒ³ã‚°ã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚
ã“ã‚Œã¾ã§ã®è­°è«–ã‚’è¸ã¾ãˆã€ã‚ãªãŸã®å°‚é–€åˆ†é‡ã§ã‚ã‚‹ã€Œ{current_expert.description}ã€ã®è¦³ç‚¹ã‹ã‚‰ã€ãƒ¦ãƒ‹ãƒ¼ã‚¯ã§å»ºè¨­çš„ãªæ„è¦‹ã€ã‚¢ã‚¤ãƒ‡ã‚¢ã€ã¾ãŸã¯æ‰¹åˆ¤çš„è¦–ç‚¹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

{history}

ã‚ãªãŸã®æ„è¦‹ã‚’ç°¡æ½”ã«è¿°ã¹ã¦ãã ã•ã„ã€‚
"""

    def _create_synthesis_prompt(self, original_prompt: str, history: str) -> str:
        """æœ€çµ‚çš„ãªçµè«–ã‚’çµ±åˆã™ã‚‹ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã™ã‚‹"""
        return f"""ä»¥ä¸‹ã®è­°é¡Œã«é–¢ã™ã‚‹å°‚é–€å®¶ãŸã¡ã®ãƒ–ãƒ¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒŸãƒ³ã‚°ã®å…¨è¨˜éŒ²ã§ã™ã€‚

{history}

# ã‚ãªãŸã¸ã®æœ€çµ‚æŒ‡ç¤º
ã“ã®è­°è«–å…¨ä½“ã‚’æ³¨æ„æ·±ãåˆ†æã—ã€ä»¥ä¸‹ã®ç‚¹ã‚’æº€ãŸã™æœ€çµ‚çš„ãªçµè«–ã‚’ä¸€ã¤ã«ã¾ã¨ã‚ã¦ãã ã•ã„:
1.  **çµ±åˆ:** å€‹ã€…ã®æ„è¦‹ã‚’ãŸã ä¸¦ã¹ã‚‹ã®ã§ã¯ãªãã€ãã‚Œã‚‰ã‚’çµ±åˆã—ã¦æ–°ã—ã„ä¸€ã¤ã®æ´å¯Ÿï¼ˆå‰µç™ºçš„ã‚¢ã‚¤ãƒ‡ã‚¢ï¼‰ã‚’å½¢æˆã—ã¦ãã ã•ã„ã€‚
2.  **é©æ–°æ€§:** æœ€ã‚‚é©æ–°çš„ã§ã€å…ƒã®è­°é¡Œã«å¯¾ã™ã‚‹ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªè§£æ±ºç­–ã‚„è¦–ç‚¹ã‚’å¼·èª¿ã—ã¦ãã ã•ã„ã€‚
3.  **æ˜ç¢ºæ€§:** èª°ãŒèª­ã‚“ã§ã‚‚ç†è§£ã§ãã‚‹ã‚ˆã†ã«ã€æ˜ç¢ºã‹ã¤ç°¡æ½”ãªè¨€è‘‰ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚

æœ€çµ‚çš„ãªçµè«–ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""

    def _find_expert(self, name: str, experts: List[ExpertModel]) -> Optional[ExpertModel]:
        """ç‰¹å®šã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’è¦‹ã¤ã‘ã‚‹ã€‚"""
        for expert in experts:
            if expert.name.lower() == name.lower():
                return expert
        return None
