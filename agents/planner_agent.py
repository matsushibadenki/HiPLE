# path: ./agents/planner_agent.py
# title: Hierarchical PlannerAgent with Consultation Step
# description: ã‚¿ã‚¹ã‚¯ã®è¤‡é›‘ã•ã«å¿œã˜ã¦ã€ä»–ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã¸ã®ç›¸è«‡(consultation)ã‚’å«ã‚€éšŽå±¤çš„ãªè¨ˆç”»ã‚’ç”Ÿæˆã™ã‚‹ã€‚

import json
import re
from typing import List, Optional
from llama_cpp.llama_types import ChatCompletionRequestMessage
from domain.schemas import Plan, SubTask, ExpertModel, Milestone
from agents.base_agent import BaseAgent

class PlannerAgent(BaseAgent):
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã‚’åˆ†æžã—ã€éšŽå±¤çš„ãªè¨ˆç”»ï¼ˆPlanï¼‰ã‚’ç”Ÿæˆã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ (HiPLE-P)
    """
    def execute(
        self,
        prompt: str,
        experts: List[ExpertModel],
        failed_plan: Optional[Plan] = None,
        validation_error: Optional[str] = None
    ) -> Plan:
        planner_expert = self._find_planner_expert(experts)
        expert_descriptions = self._format_expert_descriptions(experts)

        system_prompt = self._build_system_prompt(expert_descriptions)
        user_prompt = self._build_user_prompt(prompt, validation_error, failed_plan)
        
        messages: List[ChatCompletionRequestMessage] = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        raw_response = self._query_llm(planner_expert, messages)
        
        return self._parse_plan_from_response(raw_response, prompt, planner_expert)

    def _find_planner_expert(self, experts: List[ExpertModel]) -> ExpertModel:
        # æ€è€ƒã®ä¸­å¿ƒã¯HRMãŒæ‹…ã†
        for expert in experts:
            if expert.name.lower() == "hrm":
                return expert
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        for expert in experts:
            if expert.chat_format != "diffusion": return expert
        raise ValueError("åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    def _build_system_prompt(self, expert_descriptions: str) -> str:
        # f-stringã¨è¤‡é›‘ãªè¤‡æ•°è¡Œæ–‡å­—åˆ—ã®æ··åœ¨ã‚’é¿ã‘ã€å®‰å…¨ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹
        prompt_header = """ã‚ãªãŸã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ›–æ˜§ãªè¦æ±‚ã‚’æ§‹é€ åŒ–ã•ã‚ŒãŸéšŽå±¤çš„è¨ˆç”»ã«å¤‰æ›ã™ã‚‹ã€è¶…å„ªç§€ãªAIãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒžãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã§ã™ã€‚

# ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã‚’åˆ†æžã—ã€ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦JSONå½¢å¼ã®å®Ÿè¡Œè¨ˆç”»ã‚’ç«‹æ¡ˆã—ã¦ãã ã•ã„ã€‚

1.  **éšŽå±¤åŒ–**: æ€è€ƒã‚’3ã¤ã®ãƒ¬ãƒ™ãƒ«ï¼ˆL1: å…¨ä½“ç›®æ¨™, L2: ãƒžã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³, L3: ã‚µãƒ–ã‚¿ã‚¹ã‚¯ï¼‰ã«åˆ†è§£ã—ã¾ã™ã€‚
2.  **æ„å‘³æ§‹é€ ã®å®šç¾© (æœ€é‡è¦)**: å„ã‚µãƒ–ã‚¿ã‚¹ã‚¯ï¼ˆL3ï¼‰ã«ã¯ã€ãã®ã‚¿ã‚¹ã‚¯ã®æœ¬è³ªçš„ãªæ„å‘³ã‚’å‡ç¸®ã—ãŸçŸ­ã„èª¬æ˜Žæ–‡ `ssv_description` ã‚’å¿…ãšè¨­å®šã—ã¦ãã ã•ã„ã€‚ã“ã‚Œã¯å¾Œç¶šã®AIãŒã‚¿ã‚¹ã‚¯ã®æ„å›³ã‚’æ­£ç¢ºã«ç†è§£ã™ã‚‹ãŸã‚ã®ã€Œæ„å‘³ã®æ ¸ã€ã¨ãªã‚Šã¾ã™ã€‚
3.  **ã‚³ãƒ³ã‚µãƒ«ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³**: ã‚¿ã‚¹ã‚¯ã®å“è³ªå‘ä¸Šã®ãŸã‚ã€è¤‡æ•°ã®å°‚é–€çŸ¥è­˜ãŒå¿…è¦ã ã¨åˆ¤æ–­ã—ãŸå ´åˆã€`consultation_experts`ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«åŠ©è¨€ã‚’æ±‚ã‚ã‚‹ã¹ãã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆåã®ãƒªã‚¹ãƒˆã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚ä¾‹ãˆã°ã€ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨ç”»åƒç”ŸæˆãŒçµ¡ã‚€ã‚¿ã‚¹ã‚¯ã§ã¯ã€ä¸»æ‹…å½“ã‚’'Transformer'ã¨ã—ã€'Visualizer'ã«åŠ©è¨€ã‚’æ±‚ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
"""

        experts_section = f"""
# åˆ©ç”¨å¯èƒ½ãªã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ
{expert_descriptions}
"""

        json_format_section = """
# JSONå‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆ (åŽ³å®ˆ)
```json
{
  "overall_goal": "ï¼ˆL1: ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ã‚’ä¸€æ–‡ã§è¡¨ç¾ã—ãŸæœ€çµ‚ç›®æ¨™ï¼‰",
  "milestones": [
    {
      "milestone_id": 1,
      "title": "ï¼ˆL2: æœ€åˆã®ãƒžã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³ã®ã‚¿ã‚¤ãƒˆãƒ«ï¼‰",
      "description": "ï¼ˆã“ã®ãƒžã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³ã®ç›®çš„ï¼‰"
    }
  ],
  "tasks": [
    {
      "task_id": 1,
      "milestone_id": 1,
      "description": "ï¼ˆL3: å®Ÿè¡Œã™ã¹ãå…·ä½“çš„ãªã‚¿ã‚¹ã‚¯å†…å®¹ï¼‰",
      "expert_name": "ï¼ˆæ‹…å½“ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆåï¼‰",
      "ssv_description": "ï¼ˆã‚¿ã‚¹ã‚¯ã®æ„å‘³ã®æ ¸ã‚’è¨˜è¿°ã—ãŸçŸ­ã„èª¬æ˜Žæ–‡ï¼‰",
      "consultation_experts": ["ï¼ˆåŠ©è¨€ã‚’æ±‚ã‚ã‚‹ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆå1ï¼‰", "ï¼ˆåŠ©è¨€ã‚’æ±‚ã‚ã‚‹ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆå2ï¼‰"],
      "dependencies": []
    }
  ]
}
```
"""

        rules_section = """
# ãƒ«ãƒ¼ãƒ«
- **ID**: `milestone_id`ã¨`task_id`ã¯1ã‹ã‚‰å§‹ã¾ã‚‹é€£ç•ªã«ã—ã¦ãã ã•ã„ã€‚
- **ä¾å­˜é–¢ä¿‚**: `dependencies`ã«ã¯å…ˆè¡Œã‚¿ã‚¹ã‚¯ã®`task_id`ã‚’ãƒªã‚¹ãƒˆã§æŒ‡å®šã—ã¾ã™ã€‚
- **å ±å‘Šã‚¿ã‚¹ã‚¯**: è¤‡é›‘ãªè¦æ±‚ã®å ´åˆã€æœ€å¾Œã«'Reporter'ã‚’é…ç½®ã—ã€æœ€çµ‚å ±å‘Šæ›¸ã‚’ä½œæˆã•ã›ã¦ãã ã•ã„ã€‚
- **å˜ç´”ãªè¦æ±‚**: å˜ç´”ãªæŒ¨æ‹¶ã‚„è³ªå•ã®å ´åˆã€ãƒžã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³ã¯1ã¤ã€ã‚¿ã‚¹ã‚¯ã‚‚1ã¤ã ã‘ç”Ÿæˆã—ã€`consultation_experts`ã¯ç©ºã®ãƒªã‚¹ãƒˆ`[]`ã«ã—ã¾ã™ã€‚
"""
        return prompt_header + experts_section + json_format_section + rules_section

    def _build_user_prompt(self, prompt: str, validation_error: Optional[str], failed_plan: Optional[Plan]) -> str:
        user_prompt = f"ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ã«å¯¾ã™ã‚‹éšŽå±¤çš„å®Ÿè¡Œè¨ˆç”»ã‚’JSONå½¢å¼ã§ä½œæˆã—ã¦ãã ã•ã„:\n\nè¦æ±‚: \"{prompt}\""
        if validation_error:
            user_prompt += f"\n\n#ã€æœ€é‡è¦ã€‘å‰å›žã®è¨ˆç”»ã¯ä»¥ä¸‹ã®æ¤œè¨¼ã‚¨ãƒ©ãƒ¼ã§å¤±æ•—ã—ã¾ã—ãŸã€‚ã“ã®ã‚¨ãƒ©ãƒ¼ã‚’å®Œå…¨ã«ä¿®æ­£ã—ã€è«–ç†çš„ã«ä¸€è²«ã—ãŸæ–°ã—ã„è¨ˆç”»ã‚’ç«‹ã¦ç›´ã—ã¦ãã ã•ã„ã€‚\nã‚¨ãƒ©ãƒ¼å†…å®¹: {validation_error}"
        if failed_plan:
            user_prompt += f"\n\n# è­¦å‘Š\nå‰å›žã®è¨ˆç”»ã¯å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸã€‚å†…å®¹ã‚’æ ¹æœ¬çš„ã«è¦‹ç›´ã—ã€æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§è¨ˆç”»ã‚’ç«‹ã¦ç›´ã—ã¦ãã ã•ã„ã€‚"
        return user_prompt

    def _parse_plan_from_response(self, raw_response: str, original_prompt: str, planner_expert: ExpertModel) -> Plan:
        try:
            print(f"--- Hierarchical Planner Raw Response ---\n{raw_response}\n--------------------------")
            json_match = re.search(r'```json\s*(\{[\s\S]*?\})\s*```', raw_response, re.DOTALL)
            if not json_match:
                start_index = raw_response.find('{')
                end_index = raw_response.rfind('}')
                if start_index == -1 or end_index == -1:
                    raise json.JSONDecodeError("å¿œç­”ã«JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚", raw_response, 0)
                plan_json_str = raw_response[start_index:end_index + 1]
            else:
                plan_json_str = json_match.group(1)
            
            plan_data = json.loads(plan_json_str)

            milestones = [Milestone(**m) for m in plan_data.get("milestones", [])]
            tasks_data = plan_data.get("tasks", [])
            if not tasks_data: # tasksãŒç©ºã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                 raise ValueError("è¨ˆç”»ã«ã‚¿ã‚¹ã‚¯ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

            for t in tasks_data:
                if "ssv_description" not in t:
                    t["ssv_description"] = t["description"] # SSVãŒãªã„å ´åˆã¯descriptionã§ä»£ç”¨
                if "consultation_experts" not in t:
                    t["consultation_experts"] = [] # consultation_expertsãŒãªã„å ´åˆã¯ç©ºãƒªã‚¹ãƒˆã§ä»£ç”¨


            tasks = [SubTask(**t) for t in tasks_data]

            return Plan(
                original_prompt=original_prompt,
                overall_goal=plan_data.get("overall_goal", "N/A"),
                milestones=milestones,
                tasks=tasks
            )
        except (json.JSONDecodeError, TypeError, ValueError, AttributeError) as e:
            print(f"âŒ éšŽå±¤çš„è¨ˆç”»ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            print(f"ðŸ” ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå…ƒã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç›´æŽ¥å®Ÿè¡Œã—ã¾ã™ã€‚")
            task = SubTask(
                task_id=1,
                milestone_id=1,
                description=original_prompt,
                expert_name=planner_expert.name,
                ssv_description=original_prompt, # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ã‚‚SSVã‚’è¨­å®š
                consultation_experts=[],
                dependencies=[]
            )
            milestone = Milestone(milestone_id=1, title="Direct Execution", description="Execute the user's prompt directly.")
            return Plan(
                original_prompt=original_prompt,
                overall_goal=original_prompt,
                milestones=[milestone],
                tasks=[task]
            )

    def _format_expert_descriptions(self, experts: List[ExpertModel]) -> str:
        return "\n".join([f"- **{e.name}**: {e.description}" for e in experts if e.name.lower() != "reporter"])
