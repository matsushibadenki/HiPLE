# path: ./agents/generator_agent.py
# title: GeneratorAgent with Execution Strategy
# description: ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®å®Ÿè¡Œæˆ¦ç•¥ã«å¿œã˜ã¦ã€ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã¨ãƒ¯ãƒ¼ã‚«ãƒ¼å®Ÿè¡Œã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹ã€‚

import os
import uuid
import traceback
from typing import List, Dict, Any, cast, Optional
from llama_cpp.llama_types import ChatCompletionRequestMessage
from domain.schemas import SubTask, ExpertModel, Milestone
from services.model_loader import ModelLoaderService # ä¿®æ­£
from services.worker_manager import WorkerManagerService, WorkerExecutionError # ä¿®æ­£
from agents.base_agent import BaseAgent
from diffusers import DiffusionPipeline

class GeneratorAgent(BaseAgent):
    """
    ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®å®Ÿè¡Œæˆ¦ç•¥ã«å¿œã˜ã¦ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ (HiPLE-G)
    """
    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
    def __init__(self, model_loader: ModelLoaderService, worker_manager: WorkerManagerService):
        super().__init__(model_loader)
        self.worker_manager = worker_manager
    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸

    def execute(self, task: SubTask, expert: ExpertModel, context: Dict[str, Any]) -> str:
        
        if expert.chat_format == "diffusion":
            return self._generate_image(expert, task.description)
        
        messages = self._build_messages_with_context(task, expert, context)

        # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
        try:
            if expert.execution_strategy == "worker":
                print(f"ğŸ”© ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹çµŒç”±ã§ '{expert.name}' ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
                response_data = self.worker_manager.invoke_llm_worker(expert, messages)
                
                if (
                    "choices" in response_data and response_data["choices"] and
                    "message" in response_data["choices"][0] and
                    "content" in response_data["choices"][0]["message"]
                ):
                    return response_data["choices"][0]["message"]["content"].strip()
                else:
                    raise WorkerExecutionError(f"ãƒ¯ãƒ¼ã‚«ãƒ¼ã‹ã‚‰ã®å¿œç­”å½¢å¼ãŒä¸æ­£ã§ã™: {response_data}")
            else:
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
                return self._query_llm(expert, messages)
        except WorkerExecutionError as e:
            print(f"âŒ ãƒ¯ãƒ¼ã‚«ãƒ¼ã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return f"ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ '{expert.name}' ã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
        # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸

    def _build_messages_with_context(self, task: SubTask, expert: ExpertModel, context: Dict[str, Any]) -> List[ChatCompletionRequestMessage]:
        milestone: Optional[Milestone] = context.get('milestone')
        
        if milestone and milestone.title == "Direct Task":
            return [
                {"role": "system", "content": expert.system_prompt},
                {"role": "user", "content": task.description}
            ]
        
        user_prompt = f"""# å…¨ä½“ç›®æ¨™ (L1)\n{context.get('overall_goal')} ...""" # çœç•¥
        # ... (æ—¢å­˜ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ãƒ­ã‚¸ãƒƒã‚¯ã¯å¤‰æ›´ãªã—)
        
        return [
            {"role": "system", "content": expert.system_prompt},
            {"role": "user", "content": user_prompt}
        ]

    def _generate_image(self, expert: ExpertModel, prompt: str) -> str:
        # ... (å¤‰æ›´ãªã—)
        return ""