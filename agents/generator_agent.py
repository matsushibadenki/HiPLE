# path: ./agents/generator_agent.py
# title: GeneratorAgent with Self-Correction, Tool-Use, and Debug Logging
# description: ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®å¿œç­”ã¨è‡ªå·±è©•ä¾¡ã‚’å—ã‘å–ã‚Šã€ã‚¿ã‚¹ã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«è¨˜éŒ²ã™ã‚‹ã€‚ã¾ãŸã€è‡ªå¾‹çš„ã«ãƒ„ãƒ¼ãƒ«åˆ©ç”¨ã‚’åˆ¤æ–­ã—è¦æ±‚ã™ã‚‹ã€‚ãƒ‡ãƒãƒƒã‚°ç”¨ã®ãƒ­ã‚°å‡ºåŠ›æ©Ÿèƒ½ã‚’å«ã‚€ã€‚

import os
import uuid
import traceback
import json
import re
from typing import List, Dict, Any, cast, Optional, Union
from llama_cpp.llama_types import ChatCompletionRequestMessage
from domain.schemas import SubTask, ExpertModel, Milestone
from services.model_loader import ModelLoaderService
from services.worker_manager import WorkerManagerService, WorkerExecutionError
from agents.base_agent import BaseAgent
from agents.consultant_agent import ConsultantAgent
from diffusers import DiffusionPipeline

class GeneratorAgent(BaseAgent):
    """
    ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®å®Ÿè¡Œæˆ¦ç•¥ã«å¿œã˜ã¦ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã—ã€è‡ªå·±è©•ä¾¡ã‚’è¨˜éŒ²ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ (HiPLE-G)
    """
    def __init__(self, model_loader: ModelLoaderService, worker_manager: WorkerManagerService, consultant_agent: ConsultantAgent):
        super().__init__(model_loader)
        self.worker_manager = worker_manager
        self.consultant_agent = consultant_agent

    def execute(self, task: SubTask, expert: ExpertModel, context: Dict[str, Any], all_experts: List[ExpertModel]) -> Dict[str, Any]:
        
        # Step 1: Plannerã‹ã‚‰ã®æ˜ç¤ºçš„ãªãƒ„ãƒ¼ãƒ«åˆ©ç”¨æŒ‡ç¤ºã‚’ãƒã‚§ãƒƒã‚¯
        tool_match = re.search(r"ãƒ„ãƒ¼ãƒ«\s*`([^`]+)`\s*ã‚’ä½¿ã£ã¦ã€Œ([^ã€]+)ã€", task.description)
        if tool_match:
            tool_name = tool_match.group(1).strip()
            tool_query = tool_match.group(2).strip()
            print(f"ğŸ› ï¸ è¨ˆç”»ã‹ã‚‰ã®æ˜ç¤ºçš„ãªãƒ„ãƒ¼ãƒ«åˆ©ç”¨è¦æ±‚ã‚’æ¤œçŸ¥: {tool_name}('{tool_query}')")
            return {
                "status": "tool_request",
                "tool_name": tool_name,
                "tool_query": tool_query,
                "tool_url": None 
            }

        # Step 2: Handle image generation if it's a diffusion model
        if expert.chat_format == "diffusion":
            result = self._generate_image(expert, task.description)
            task.self_evaluation = {"confidence": 0.9, "reasoning": "Image generated."}
            return {"status": "completed", "result": result}
        
        # Step 3: Handle normal text generation tasks
        consultation_feedback = ""
        if task.consultation_experts:
            consultation_result = self.consultant_agent.execute(
                original_task=task,
                primary_expert=expert,
                all_experts=all_experts
            )
            consultation_feedback = consultation_result.get("response", "")
        
        messages = self._build_messages_with_context(task, expert, context, consultation_feedback)
        
        print(f"\n[GeneratorAgent] ğŸ“ LLMã«é€ä¿¡ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ (æ‹…å½“: {expert.name}):")
        for i, msg in enumerate(messages):
            print(f"  - Message {i+1} Role: {msg['role']}")
            content_preview = str(msg['content']).replace('\n', ' ').strip()
            print(f"    Content (Preview): {content_preview[:400]}...")

        response_data: Dict[str, Any] = {}
        try:
            if expert.execution_strategy == "worker":
                response_dict_from_worker = self.worker_manager.invoke_llm_worker(expert, messages)
                raw_response_str = response_dict_from_worker.get("choices", [{}])[0].get("message", {}).get("content", "")
                print(f"\n[GeneratorAgent] ğŸ¤– Worker LLMã‹ã‚‰ã®ç”Ÿå¿œç­” (æ‹…å½“: {expert.name}):\n---\n{raw_response_str}\n---")
                try:
                    parsed_data = self._parse_self_evaluation_from_str(raw_response_str)
                    response_data = parsed_data
                except (json.JSONDecodeError, KeyError):
                    response_data = {"response": raw_response_str, "self_evaluation": {"confidence": 0.75, "reasoning": "Evaluation from worker could not be parsed."}}
            else:
                response_data = self._query_llm(expert, messages)
                print(f"\n[GeneratorAgent] ğŸ¤– LLMã‹ã‚‰ã®ç”Ÿå¿œç­” (æ‹…å½“: {expert.name}):\n---\n{response_data}\n---")
        except WorkerExecutionError as e:
            print(f"âŒ ãƒ¯ãƒ¼ã‚«ãƒ¼ã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return {"status": "failed", "result": f"ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ '{expert.name}' ã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"}

        raw_response = response_data.get("response", "")
        task.self_evaluation = response_data.get("self_evaluation")

        if isinstance(raw_response, dict) and "tool_use" in raw_response:
            tool_info = raw_response["tool_use"]
            if isinstance(tool_info, dict) and "tool_name" in tool_info and "tool_query" in tool_info:
                print(f"ğŸ› ï¸ ãƒ„ãƒ¼ãƒ«åˆ©ç”¨è¦æ±‚ã‚’æ¤œçŸ¥: {tool_info['tool_name']}('{tool_info['tool_query']}')")
                return {
                    "status": "tool_request",
                    "tool_name": tool_info["tool_name"],
                    "tool_query": tool_info["tool_query"],
                    "tool_url": tool_info.get("tool_url")
                }

        result_str = str(raw_response) if isinstance(raw_response, dict) else raw_response
        return {"status": "completed", "result": result_str.strip()}

    def _parse_self_evaluation_from_str(self, raw_str: str) -> Dict[str, Any]:
        """ æ–‡å­—åˆ—ã‹ã‚‰è‡ªå·±è©•ä¾¡JSONã‚’ãƒ‘ãƒ¼ã‚¹ã™ã‚‹ """
        json_match = re.search(r'```json\s*(\{[\s\S]*?\})\s*```', raw_str, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        else:
            start_index = raw_str.find('{')
            end_index = raw_str.rfind('}')
            if start_index != -1 and end_index != -1:
                json_str = raw_str[start_index:end_index + 1]
            else:
                raise json.JSONDecodeError("No JSON object found", raw_str, 0)
        
        data = json.loads(json_str)
        if "response" not in data or "self_evaluation" not in data:
            raise KeyError("Missing 'response' or 'self_evaluation' key")
        return data

    def _build_messages_with_context(
        self,
        task: SubTask,
        expert: ExpertModel,
        context: Dict[str, Any],
        consultation_feedback: str = ""
    ) -> List[ChatCompletionRequestMessage]:
        
        milestone: Optional[Milestone] = context.get('milestone')
        system_prompt = expert.system_prompt
        dependency_results = context.get("dependency_results", "")
        rag_results_list = context.get("rag_results", [])
        rag_results_str = "\n".join([f"- {r}" for r in rag_results_list])
        ssv_description = context.get('ssv_description', task.description)
        tool_results = context.get("tool_results", "")
        feedback = task.feedback_history[-1].get("feedback") if task.feedback_history else ""

        if tool_results:
            main_instruction = """# ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯ (L3)
å…ˆè¡Œã‚¿ã‚¹ã‚¯ã«ã‚ˆã£ã¦ã€ä»¥ä¸‹ã®ã€Œãƒ„ãƒ¼ãƒ«ã‹ã‚‰ã®æƒ…å ±ã€ãŒåé›†ã•ã‚Œã¾ã—ãŸã€‚
ã“ã®æƒ…å ±ã‚’åŸºã«ã€ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯è©³ç´°ã‚’é”æˆã™ã‚‹ãŸã‚ã®å¿œç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
"""
        else:
            # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
            main_instruction = r"""# ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯ (L3)
ä»¥ä¸Šã®å…¨ã¦ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’è¸ã¾ãˆã€ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚

**ã€æœ€é‡è¦ã€‘**
**ã‚‚ã—ã€ã“ã®ã‚¿ã‚¹ã‚¯ã‚’é”æˆã™ã‚‹ãŸã‚ã«å¤–éƒ¨æƒ…å ±ï¼ˆWebæ¤œç´¢ã‚„Wikipediaï¼‰ãŒå¿…è¦ã ã¨åˆ¤æ–­ã—ãŸå ´åˆ**ã€ä»–ã®å¿œç­”ã¯ä¸€åˆ‡ã›ãšã€å¿…ãšä»¥ä¸‹ã®JSONå½¢å¼ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

```json
{
  "response": {
    "tool_use": {
      "tool_name": "ï¼ˆ'web_search'ã¾ãŸã¯'wikipedia_search'ï¼‰",
      "tool_query": "ï¼ˆæ¤œç´¢ã‚„å®Ÿè¡Œã®ãŸã‚ã®æœ€ã‚‚å…·ä½“çš„ã§åŠ¹æœçš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„è³ªå•ï¼‰"
    }
  },
  "self_evaluation": {
    "confidence": 1.0,
    "reasoning": "This task requires external information that can be obtained by using a tool."
  }
}
```
**ãƒ„ãƒ¼ãƒ«ãŒä¸è¦ãªå ´åˆã«ã®ã¿**ã€é€šå¸¸ã®å¿œç­”ã¨è‡ªå·±è©•ä¾¡ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
"""
            # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
        
        user_prompt = f"""# å…¨ä½“ç›®æ¨™ (L1)
{context.get('overall_goal', 'N/A')}

# ç¾åœ¨ã®ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³ (L2)
ã‚¿ã‚¤ãƒˆãƒ«: {milestone.title if milestone else 'N/A'}
èª¬æ˜: {milestone.description if milestone else 'N/A'}

# å…ˆè¡Œã‚¿ã‚¹ã‚¯ã‹ã‚‰ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
{dependency_results if dependency_results else "å…ˆè¡Œã‚¿ã‚¹ã‚¯ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"}

# é–¢é€£æƒ…å ± (RAG)
{rag_results_str if rag_results_str else "é–¢é€£æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"}

# ãƒ„ãƒ¼ãƒ«ã‹ã‚‰ã®æƒ…å ±
{tool_results if tool_results else "ãƒ„ãƒ¼ãƒ«ã‹ã‚‰ã®æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"}

# å°‚é–€å®¶ã‹ã‚‰ã®åŠ©è¨€ (ã‚³ãƒ³ã‚µãƒ«ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³)
{consultation_feedback if consultation_feedback else "ç‰¹ã«ãªã—ã€‚"}

#ã€é‡è¦ã€‘å‰å›ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‹ã‚‰ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
{feedback if feedback else "ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"}

{main_instruction}

## ã‚¿ã‚¹ã‚¯ã®æ ¸å¿ƒ (SSV)
**ã“ã®ã‚¿ã‚¹ã‚¯ã§æœ€ã‚‚é‡è¦ãªç›®çš„ã¯ã€Œ{ssv_description}ã€ã‚’é”æˆã™ã‚‹ã“ã¨ã§ã™ã€‚**

## ã‚¿ã‚¹ã‚¯ã®è©³ç´°
{task.description}
"""
        
        return [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

    def _generate_image(self, expert: ExpertModel, prompt: str) -> str:
        print(f"ğŸ¨ æ‹¡æ•£ãƒ¢ãƒ‡ãƒ« '{expert.name}' ã‚’ä½¿ç”¨ã—ã¦ç”»åƒã‚’ç”Ÿæˆã—ã¾ã™...")
        try:
            pipe = cast(DiffusionPipeline, self.model_loader.load_expert(expert))
            image = pipe(prompt=prompt).images[0]
            
            output_dir = "output/images"
            os.makedirs(output_dir, exist_ok=True)
            
            filename = f"{uuid.uuid4()}.png"
            output_path = os.path.join(output_dir, filename)
            image.save(output_path)
            
            absolute_path = os.path.abspath(output_path)
            print(f"ğŸ–¼ï¸ ç”»åƒã‚’ä¿å­˜ã—ã¾ã—ãŸ: {absolute_path}")
            return f"ç”»åƒã‚’ {absolute_path} ã«ç”Ÿæˆã—ã¾ã—ãŸã€‚"

        except Exception as e:
            error_message = f"ã‚¨ãƒ©ãƒ¼: ç”»åƒã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ - {e}"
            print(f"âŒ {error_message}")
            traceback.print_exc()
            return error_message
