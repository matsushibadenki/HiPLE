# path: ./agents/generator_agent.py
# title: GeneratorAgent with Tool Handling
# description: ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®å¿œç­”ã‚’è§£æã—ã€ãƒ„ãƒ¼ãƒ«åˆ©ç”¨è¦æ±‚ã‚’æ¤œçŸ¥ã—ã¦ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã«å‡¦ç†ã‚’å§”è­²ã™ã‚‹ã€‚

import os
import uuid
import traceback
import json
import re
from typing import List, Dict, Any, cast, Optional, Union
from llama_cpp.llama_types import ChatCompletionRequestMessage
from domain.schemas import SubTask, ExpertModel, Milestone
from services.model_loader import ModelLoaderService
from services.worker_manager import WorkerManagerService, WorkerExecutionError
from agents.base_agent import BaseAgent
from agents.consultant_agent import ConsultantAgent
from diffusers import DiffusionPipeline

class GeneratorAgent(BaseAgent):
    """
    ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®å®Ÿè¡Œæˆ¦ç•¥ã«å¿œã˜ã¦ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ (HiPLE-G)
    ãƒ„ãƒ¼ãƒ«åˆ©ç”¨è¦æ±‚ã‚’æ¤œçŸ¥ã—ã€ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã«å‡¦ç†ã‚’å§”è­²ã™ã‚‹ã€‚
    """
    def __init__(self, model_loader: ModelLoaderService, worker_manager: WorkerManagerService, consultant_agent: ConsultantAgent):
        super().__init__(model_loader)
        self.worker_manager = worker_manager
        self.consultant_agent = consultant_agent

    def execute(self, task: SubTask, expert: ExpertModel, context: Dict[str, Any], all_experts: List[ExpertModel]) -> Dict[str, Any]:
        
        if expert.chat_format == "diffusion":
            result = self._generate_image(expert, task.description)
            return {"status": "completed", "result": result}
        
        consultation_feedback = ""
        if task.consultation_experts:
            consultation_feedback = self.consultant_agent.execute(
                original_task=task,
                primary_expert=expert,
                all_experts=all_experts
            )
        
        messages = self._build_messages_with_context(task, expert, context, consultation_feedback)
        
        raw_response = ""
        try:
            if expert.execution_strategy == "worker":
                response_data = self.worker_manager.invoke_llm_worker(expert, messages)
                if ("choices" in response_data and response_data["choices"] and
                    "message" in response_data["choices"][0] and
                    "content" in response_data["choices"][0]["message"]):
                    raw_response = response_data["choices"][0]["message"]["content"] or ""
                else:
                    raise WorkerExecutionError(f"ãƒ¯ãƒ¼ã‚«ãƒ¼ã‹ã‚‰ã®å¿œç­”å½¢å¼ãŒä¸æ­£ã§ã™: {response_data}")
            else:
                raw_response = self._query_llm(expert, messages)
        except WorkerExecutionError as e:
            print(f"âŒ ãƒ¯ãƒ¼ã‚«ãƒ¼ã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return {"status": "failed", "result": f"ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ '{expert.name}' ã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"}

        tool_use_match = re.search(r'```json\s*(\{[\s\S]*?"tool_use"[\s\S]*?\})\s*```', raw_response, re.DOTALL)
        if tool_use_match:
            try:
                tool_request = json.loads(tool_use_match.group(1))
                tool_info = tool_request.get("tool_use", {})
                tool_name = tool_info.get("tool_name")
                tool_query = tool_info.get("tool_query")
                tool_url = tool_info.get("tool_url")

                if tool_name and tool_query:
                    print(f" à¦Ÿà§à¦²åˆ©ç”¨è¦æ±‚ã‚’æ¤œçŸ¥: {tool_name}('{tool_query}')")
                    return {
                        "status": "tool_request",
                        "tool_name": tool_name,
                        "tool_query": tool_query,
                        "tool_url": tool_url
                    }
            except json.JSONDecodeError:
                pass 

        return {"status": "completed", "result": raw_response.strip()}

    def _build_messages_with_context(
        self,
        task: SubTask,
        expert: ExpertModel,
        context: Dict[str, Any],
        consultation_feedback: str = ""
    ) -> List[ChatCompletionRequestMessage]:
        milestone: Optional[Milestone] = context.get('milestone')
        
        tool_results = context.get("tool_results", "")
        system_prompt = self._add_tool_use_prompt_to_system(expert.system_prompt)

        dependency_results = context.get("dependency_results", "")
        rag_results_list = context.get("rag_results", [])
        rag_results_str = "\n".join([f"- {r}" for r in rag_results_list])
        ssv_description = context.get('ssv_description', task.description)

        user_prompt = f"""# å…¨ä½“ç›®æ¨™ (L1)
{context.get('overall_goal', 'N/A')}

# ç¾åœ¨ã®ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³ (L2)
ã‚¿ã‚¤ãƒˆãƒ«: {milestone.title if milestone else 'N/A'}
èª¬æ˜: {milestone.description if milestone else 'N/A'}

# å…ˆè¡Œã‚¿ã‚¹ã‚¯ã‹ã‚‰ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
{dependency_results if dependency_results else "å…ˆè¡Œã‚¿ã‚¹ã‚¯ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"}

# é–¢é€£æƒ…å ± (RAG)
{rag_results_str if rag_results_str else "é–¢é€£æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"}

# ãƒ„ãƒ¼ãƒ«ã‹ã‚‰ã®æƒ…å ±
{tool_results if tool_results else "ãƒ„ãƒ¼ãƒ«ã‹ã‚‰ã®æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"}

# å°‚é–€å®¶ã‹ã‚‰ã®åŠ©è¨€ (ã‚³ãƒ³ã‚µãƒ«ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³)
{consultation_feedback if consultation_feedback else "ç‰¹ã«ãªã—ã€‚"}

# ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯ (L3)
ä»¥ä¸Šã®å…¨ã¦ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨å°‚é–€å®¶ã®åŠ©è¨€ã‚’è¸ã¾ãˆã€ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚
**ã‚‚ã—ã‚¿ã‚¹ã‚¯å®Ÿè¡Œã«å¤–éƒ¨ã®æƒ…å ±ãŒå¿…è¦ã ã¨åˆ¤æ–­ã—ãŸå ´åˆã€å¾Œè¿°ã®JSONå½¢å¼ã§ãƒ„ãƒ¼ãƒ«åˆ©ç”¨ã‚’è¦æ±‚ã—ã¦ãã ã•ã„ã€‚**

## ã‚¿ã‚¹ã‚¯ã®æ ¸å¿ƒ (SSV)
**ã“ã®ã‚¿ã‚¹ã‚¯ã§æœ€ã‚‚é‡è¦ãªç›®çš„ã¯ã€Œ{ssv_description}ã€ã‚’é”æˆã™ã‚‹ã“ã¨ã§ã™ã€‚**

## ã‚¿ã‚¹ã‚¯ã®è©³ç´°
{task.description}
"""
        
        return [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

    def _add_tool_use_prompt_to_system(self, original_prompt: str) -> str:
        tool_prompt = """
# ãƒ„ãƒ¼ãƒ«åˆ©ç”¨
ã‚¿ã‚¹ã‚¯ã®å®Ÿè¡Œã«å¤–éƒ¨æƒ…å ±ï¼ˆä¾‹: æœ€æ–°ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€æ™®éçš„ãªçŸ¥è­˜ï¼‰ãŒå¿…è¦ãªå ´åˆã¯ã€é€šå¸¸ã®å¿œç­”ã®ä»£ã‚ã‚Šã«ã€ä»¥ä¸‹ã®JSONå½¢å¼ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

```json
{
  "tool_use": {
    "tool_name": "ï¼ˆ'wikipedia_search' ã¾ãŸã¯ 'web_search'ï¼‰",
    "tool_query": "ï¼ˆãƒ„ãƒ¼ãƒ«ã§æ¤œç´¢ãƒ»è¦ç´„ã•ã›ãŸã„å…·ä½“çš„ãªè³ªå•æ–‡ã‚„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰",
    "tool_url": "ï¼ˆweb_searchã®å ´åˆã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã»ã—ã„URLã€‚ä¸è¦ãªå ´åˆã¯nullï¼‰",
    "reasoning": "ï¼ˆãªãœã“ã®ãƒ„ãƒ¼ãƒ«ãŒå¿…è¦ãªã®ã‹ã®ç°¡å˜ãªèª¬æ˜ï¼‰"
  }
}
```
"""
        return original_prompt + "\n" + tool_prompt

    def _generate_image(self, expert: ExpertModel, prompt: str) -> str:
        print(f"ğŸ¨ æ‹¡æ•£ãƒ¢ãƒ‡ãƒ« '{expert.name}' ã‚’ä½¿ç”¨ã—ã¦ç”»åƒã‚’ç”Ÿæˆã—ã¾ã™...")
        try:
            pipe = cast(DiffusionPipeline, self.model_loader.load_expert(expert))
            image = pipe(prompt=prompt).images[0]  # type: ignore[operator]
            
            output_dir = "output/images"
            os.makedirs(output_dir, exist_ok=True)
            
            filename = f"{uuid.uuid4()}.png"
            output_path = os.path.join(output_dir, filename)
            image.save(output_path)
            
            absolute_path = os.path.abspath(output_path)
            print(f"ğŸ–¼ï¸ ç”»åƒã‚’ä¿å­˜ã—ã¾ã—ãŸ: {absolute_path}")
            return f"ç”»åƒã‚’ {absolute_path} ã«ç”Ÿæˆã—ã¾ã—ãŸã€‚"

        except Exception as e:
            error_message = f"ã‚¨ãƒ©ãƒ¼: ç”»åƒã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ - {e}"
            print(f"âŒ {error_message}")
            traceback.print_exc()
            return error_message

