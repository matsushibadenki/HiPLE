# path: ./agents/generator_agent.py
# title: GeneratorAgent with Consultation and Execution Strategy
# description: å¿…è¦ã«å¿œã˜ã¦ã‚³ãƒ³ã‚µãƒ«ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã„ã€ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®å®Ÿè¡Œæˆ¦ç•¥ã«å¿œã˜ã¦ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚

import os
import uuid
import traceback
from typing import List, Dict, Any, cast, Optional
from llama_cpp.llama_types import ChatCompletionRequestMessage
from domain.schemas import SubTask, ExpertModel, Milestone
from services.model_loader import ModelLoaderService
from services.worker_manager import WorkerManagerService, WorkerExecutionError
from agents.base_agent import BaseAgent
from agents.consultant_agent import ConsultantAgent
from diffusers import DiffusionPipeline

class GeneratorAgent(BaseAgent):
    """
    ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®å®Ÿè¡Œæˆ¦ç•¥ã«å¿œã˜ã¦ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ (HiPLE-G)
    å¿…è¦ã«å¿œã˜ã¦ã‚³ãƒ³ã‚µãƒ«ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã†ã€‚
    """
    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
    def __init__(self, model_loader: ModelLoaderService, worker_manager: WorkerManagerService, consultant_agent: ConsultantAgent):
        super().__init__(model_loader)
        self.worker_manager = worker_manager
        self.consultant_agent = consultant_agent

    def execute(self, task: SubTask, expert: ExpertModel, context: Dict[str, Any], all_experts: List[ExpertModel]) -> str:
    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
        
        if expert.chat_format == "diffusion":
            return self._generate_image(expert, task.description)
        
        # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
        consultation_feedback = ""
        if task.consultation_experts:
            consultation_feedback = self.consultant_agent.execute(
                original_task=task,
                primary_expert=expert,
                all_experts=all_experts
            )
        
        messages = self._build_messages_with_context(task, expert, context, consultation_feedback)
        # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸

        try:
            if expert.execution_strategy == "worker":
                print(f"ğŸ”© ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹çµŒç”±ã§ '{expert.name}' ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
                response_data = self.worker_manager.invoke_llm_worker(expert, messages)
                
                if (
                    "choices" in response_data and response_data["choices"] and
                    "message" in response_data["choices"][0] and
                    "content" in response_data["choices"][0]["message"]
                ):
                    content = response_data["choices"][0]["message"]["content"]
                    return content.strip() if content else ""
                else:
                    raise WorkerExecutionError(f"ãƒ¯ãƒ¼ã‚«ãƒ¼ã‹ã‚‰ã®å¿œç­”å½¢å¼ãŒä¸æ­£ã§ã™: {response_data}")
            else:
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
                return self._query_llm(expert, messages)
        except WorkerExecutionError as e:
            print(f"âŒ ãƒ¯ãƒ¼ã‚«ãƒ¼ã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return f"ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ '{expert.name}' ã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"

    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
    def _build_messages_with_context(
        self,
        task: SubTask,
        expert: ExpertModel,
        context: Dict[str, Any],
        consultation_feedback: str = ""
    ) -> List[ChatCompletionRequestMessage]:
    # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
        milestone: Optional[Milestone] = context.get('milestone')
        
        if milestone and milestone.title == "Direct Task":
            return [
                {"role": "system", "content": expert.system_prompt},
                {"role": "user", "content": task.description}
            ]
        
        dependency_results = context.get("dependency_results", "")
        rag_results_list = context.get("rag_results", [])
        rag_results_str = "\n".join([f"- {r.get('content', '')}" for r in rag_results_list])

        ssv_description = context.get('ssv_description', task.description)

        # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
        user_prompt = f"""# å…¨ä½“ç›®æ¨™ (L1)
{context.get('overall_goal', 'N/A')}

# ç¾åœ¨ã®ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³ (L2)
ã‚¿ã‚¤ãƒˆãƒ«: {milestone.title if milestone else 'N/A'}
èª¬æ˜: {milestone.description if milestone else 'N/A'}

# å…ˆè¡Œã‚¿ã‚¹ã‚¯ã‹ã‚‰ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
{dependency_results if dependency_results else "å…ˆè¡Œã‚¿ã‚¹ã‚¯ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"}

# é–¢é€£æƒ…å ± (RAG)
{rag_results_str if rag_results_str else "é–¢é€£æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"}

# å°‚é–€å®¶ã‹ã‚‰ã®åŠ©è¨€ (ã‚³ãƒ³ã‚µãƒ«ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³)
{consultation_feedback if consultation_feedback else "ç‰¹ã«ãªã—ã€‚"}

# ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯ (L3)
ä»¥ä¸Šã®å…¨ã¦ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨å°‚é–€å®¶ã®åŠ©è¨€ã‚’è¸ã¾ãˆã€ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚

## ã‚¿ã‚¹ã‚¯ã®æ ¸å¿ƒ (SSV)
**ã“ã®ã‚¿ã‚¹ã‚¯ã§æœ€ã‚‚é‡è¦ãªç›®çš„ã¯ã€Œ{ssv_description}ã€ã‚’é”æˆã™ã‚‹ã“ã¨ã§ã™ã€‚**

## ã‚¿ã‚¹ã‚¯ã®è©³ç´°
{task.description}

---
ä¸Šè¨˜ã®æ ¸å¿ƒï¼ˆSSVï¼‰ã¨å°‚é–€å®¶ã®åŠ©è¨€ã‚’æœ€å„ªå…ˆã—ã€è©³ç´°æƒ…å ±ã‚’å‚è€ƒã«ã—ãªãŒã‚‰ã€å…·ä½“çš„ã§è³ªã®é«˜ã„æˆæœç‰©ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
"""
        # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
        
        return [
            {"role": "system", "content": expert.system_prompt},
            {"role": "user", "content": user_prompt}
        ]

    def _generate_image(self, expert: ExpertModel, prompt: str) -> str:
        """
        æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ç”»åƒã‚’ç”Ÿæˆã—ã€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æ–‡å­—åˆ—ã¨ã—ã¦è¿”ã™ã€‚
        """
        print(f"ğŸ¨ æ‹¡æ•£ãƒ¢ãƒ‡ãƒ« '{expert.name}' ã‚’ä½¿ç”¨ã—ã¦ç”»åƒã‚’ç”Ÿæˆã—ã¾ã™...")
        try:
            # model_loaderã‚’ä½¿ç”¨ã—ã¦ã€è¨­å®šã«åŸºã¥ãæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ãƒ­ãƒ¼ãƒ‰
            pipe = cast(DiffusionPipeline, self.model_loader.load_expert(expert))
            
            # ç”»åƒã‚’ç”Ÿæˆ
            # mypyãŒDiffusionPipelineãŒå‘¼ã³å‡ºã—å¯èƒ½ã§ãªã„ã¨èª¤èªã™ã‚‹ãŸã‚ã€å‹ãƒã‚§ãƒƒã‚¯ã‚’ç„¡è¦–
            image = pipe(prompt=prompt).images[0] # type: ignore[operator]
            
            # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
            output_dir = "output/images"
            os.makedirs(output_dir, exist_ok=True)
            
            # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆã—ã¦ä¿å­˜
            filename = f"{uuid.uuid4()}.png"
            output_path = os.path.join(output_dir, filename)
            image.save(output_path)
            
            absolute_path = os.path.abspath(output_path)
            print(f"ğŸ–¼ï¸ ç”»åƒã‚’ä¿å­˜ã—ã¾ã—ãŸ: {absolute_path}")
            return f"ç”»åƒã‚’ {absolute_path} ã«ç”Ÿæˆã—ã¾ã—ãŸã€‚"

        except Exception as e:
            error_message = f"ã‚¨ãƒ©ãƒ¼: ç”»åƒã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ - {e}"
            print(f"âŒ {error_message}")
            traceback.print_exc()
            return error_message