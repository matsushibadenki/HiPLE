# path: ./services/performance_tracker_service.py
# title: Performance Tracker Service
# description: AIã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è¿½è·¡ãƒ»è©•ä¾¡ã—ã€æœ€é©ãªã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’é¸æŠã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã€‚

import time
from typing import Dict, Optional, List
from domain.evaluation import PerformanceMetrics
from domain.schemas import ExpertModel

class PerformanceTrackerService:
    """
    ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è¨˜éŒ²ãƒ»ç®¡ç†ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹
    """
    def __init__(self):
        self.performance_records: Dict[str, PerformanceMetrics] = {}

    def update_performance(
        self,
        expert_name: str,
        execution_time: float,
        success: bool
    ) -> None:
        """
        æŒ‡å®šã•ã‚ŒãŸã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ›´æ–°ã™ã‚‹
        """
        if expert_name not in self.performance_records:
            self.performance_records[expert_name] = PerformanceMetrics()

        record = self.performance_records[expert_name]
        if success:
            record.success_count += 1
            record.total_execution_time += execution_time
        else:
            record.failure_count += 1

        self._recalculate_score(expert_name)
        print(f"ğŸ“Š Performance updated for '{expert_name}': Score={record.score:.2f}, SuccessRate={record.success_rate:.2f}, AvgTime={record.average_execution_time:.2f}s")


    def _recalculate_score(self, expert_name: str) -> None:
        """
        ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ã‚¹ã‚³ã‚¢ã‚’å†è¨ˆç®—ã™ã‚‹
        ã‚¹ã‚³ã‚¢ = æˆåŠŸç‡ã‚’é‡è¦–ã—ã¤ã¤ã€å®Ÿè¡Œæ™‚é–“ãŒçŸ­ã„ã»ã©é«˜è©•ä¾¡
        """
        record = self.performance_records.get(expert_name)
        if not record or record.total_runs == 0:
            return

        # å…¨ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®å¹³å‡å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆç®—ï¼ˆã‚¹ã‚³ã‚¢æ­£è¦åŒ–ã®ãŸã‚ï¼‰
        all_avg_times = [
            r.average_execution_time
            for r in self.performance_records.values()
            if r.success_count > 0
        ]
        max_avg_time = max(all_avg_times) if all_avg_times else 1.0
        
        # å®Ÿè¡Œæ™‚é–“ãƒšãƒŠãƒ«ãƒ†ã‚£ (0ã‹ã‚‰1ã®ç¯„å›²ã€æ™‚é–“ãŒé•·ã„ã»ã©1ã«è¿‘ã¥ã)
        # ã‚¼ãƒ­é™¤ç®—ã‚’é¿ã‘ã‚‹
        normalized_time = record.average_execution_time / max_avg_time if max_avg_time > 0 else 0
        time_penalty = min(normalized_time, 1.0)

        # ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆæˆåŠŸç‡ã‚’ä¸»è»¸ã«ã€å®Ÿè¡Œæ™‚é–“ãŒçŸ­ã„ã»ã©ãƒœãƒ¼ãƒŠã‚¹ï¼‰
        record.score = record.success_rate * (1.0 - (time_penalty * 0.2))


    def get_best_expert(self, experts: List[ExpertModel], task_type: str = "general") -> Optional[ExpertModel]:
        """
        ç¾åœ¨æœ€ã‚‚ã‚¹ã‚³ã‚¢ã®é«˜ã„ã€åˆ©ç”¨å¯èƒ½ãªã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’è¿”ã™
        """
        best_expert: Optional[ExpertModel] = None
        highest_score = -1.0

        # è©•ä¾¡è¨˜éŒ²ãŒã‚ã‚‹ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ã¿ã‚’å¯¾è±¡
        eligible_experts = [
            e for e in experts if e.name in self.performance_records and e.chat_format != "diffusion"
        ]

        if not eligible_experts:
            # è©•ä¾¡è¨˜éŒ²ãŒãªã„å ´åˆã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆHRMãªã©ï¼‰ã‚’è¿”ã™
            return next((e for e in experts if e.name.lower() == "hrm"), None)

        for expert in eligible_experts:
            score = self.performance_records[expert.name].score
            if score > highest_score:
                highest_score = score
                best_expert = expert
        
        # ã‚‚ã—å…¨ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ã‚¹ã‚³ã‚¢ãŒ0ãªã‚‰ã€ãƒ©ãƒ³ãƒ€ãƒ ã‚„ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if not best_expert:
             return next((e for e in experts if e.name.lower() == "hrm"), None)

        print(f"ğŸ† Best expert selected for '{task_type}': '{best_expert.name}' (Score: {highest_score:.2f})")
        return best_expert

    def get_performance_summary(self) -> str:
        """
        å…¨ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼ã‚’æ–‡å­—åˆ—ã¨ã—ã¦è¿”ã™
        """
        summary_lines = ["# Expert Performance Summary"]
        for name, record in sorted(self.performance_records.items(), key=lambda item: item[1].score, reverse=True):
            summary_lines.append(
                f"- **{name}**: "
                f"Score={record.score:.2f}, "
                f"SuccessRate={record.success_rate * 100:.1f}%, "
                f"AvgTime={record.average_execution_time:.2f}s, "
                f"Runs={record.total_runs}"
            )
        return "\n".join(summary_lines)