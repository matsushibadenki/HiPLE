# path: ./services/model_loader.py
# title: Model Loader Service with Stabilized Parameters
# description: jamba_test.pyã®è¨­å®šã‚’å‚è€ƒã«ã€ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æ™‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®‰å®šåŒ–ã•ã›ã‚‹ã€‚

import os
import gc
import sys
import traceback
from typing import Optional, Any, Union
import psutil
from llama_cpp import Llama
from domain.schemas import ExpertModel
import torch
from diffusers import DiffusionPipeline, AutoencoderKL
from utils.monitoring import print_memory_usage

class ModelLoaderService:
    """
    ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã¨ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚¹ãƒ¯ãƒƒãƒ”ãƒ³ã‚°ï¼‰ã‚’ç®¡ç†ã™ã‚‹
    """
    def __init__(self, memory_threshold_gb: int = 2) -> None:
        self.currently_loaded_expert: Optional[ExpertModel] = None
        self.memory_threshold_bytes = memory_threshold_gb * 1024 * 1024 * 1024

    def _check_memory(self) -> None:
        available_memory = psutil.virtual_memory().available
        if available_memory < self.memory_threshold_bytes:
            raise MemoryError(f"åˆ©ç”¨å¯èƒ½ãªãƒ¡ãƒ¢ãƒª ({available_memory / (1024**3):.2f}GB) ãŒé–¾å€¤ã‚’ä¸‹å›ã£ã¦ã„ã¾ã™ã€‚")
        print(f"âœ… ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯OK (Available: {available_memory / (1024**3):.2f}GB)")

    def load_expert(self, expert: ExpertModel) -> Union[Llama, DiffusionPipeline]:
        if expert.is_loaded and expert.instance:
            if self.currently_loaded_expert and self.currently_loaded_expert.name != expert.name:
                self.unload_expert(self.currently_loaded_expert)
            self.currently_loaded_expert = expert
            print(f"âœ… ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ '{expert.name}' ã¯æ—¢ã«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã§ã™ã€‚")
            if isinstance(expert.instance, Llama):
                expert.instance.reset()
            return expert.instance

        if self.currently_loaded_expert:
            self.unload_expert(self.currently_loaded_expert)

        self._check_memory()
        print(f"ğŸ”„ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ '{expert.name}' ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™...")
        print_memory_usage(f"BEFORE_LOAD_{expert.name.upper()}")

        try:
            instance: Union[Llama, DiffusionPipeline]
            if expert.chat_format == "diffusion":
                if not expert.model_id: raise ValueError("æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®model_idãŒæœªè¨­å®š")
                device = "mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu"
                vae = AutoencoderKL.from_pretrained("madebyollin/sdxl-vae-fp16-fix", torch_dtype=torch.float16)
                pipe = DiffusionPipeline.from_pretrained(expert.model_id, vae=vae, torch_dtype=torch.float16, variant="fp16", use_safetensors=True)
                instance = pipe.to(device)
            else:
                if not expert.model_path: raise ValueError("LLMã®model_pathãŒæœªè¨­å®š")
                log_verbose = os.getenv("LOG_VERBOSE", "False").lower() in ("true", "1", "t")
                is_apple_silicon = sys.platform == "darwin" and "arm64" in os.uname().machine
                n_gpu_layers = -1 if is_apple_silicon else 0
                
                # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ä¿®æ­£é–‹å§‹â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸
                # jamba_test.pyã‚’å‚è€ƒã«ã€å®‰å®šå‹•ä½œã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šã«çµ±ä¸€
                n_threads = psutil.cpu_count(logical=False)
                instance = Llama(
                    model_path=expert.model_path, 
                    n_gpu_layers=n_gpu_layers, 
                    n_ctx=4096, 
                    use_mlock=True, 
                    use_mmap=False, # ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ã‚’ç„¡åŠ¹åŒ–
                    n_threads=n_threads, # ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’æ˜ç¤º
                    verbose=log_verbose, 
                    chat_format=expert.chat_format
                )
                # â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ä¿®æ­£çµ‚ã‚ã‚Šâ—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸

            expert.instance = instance
            expert.is_loaded = True
            self.currently_loaded_expert = expert
            print(f"âœ… ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ '{expert.name}' ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            print_memory_usage(f"AFTER_LOAD_{expert.name.upper()}")
            return instance
        except Exception as e:
            print(f"âŒ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ '{expert.name}' ã®ãƒ­ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            traceback.print_exc()
            raise

    def unload_expert(self, expert: Optional[ExpertModel]) -> None:
        if expert and expert.instance:
            print(f"ğŸ§¹ ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆ '{expert.name}' ã‚’ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾ã—ã¾ã™ã€‚")
            print_memory_usage(f"BEFORE_UNLOAD_{expert.name.upper()}")
            del expert.instance
            expert.instance = None
            expert.is_loaded = False
            if self.currently_loaded_expert and self.currently_loaded_expert.name == expert.name:
                self.currently_loaded_expert = None
            gc.collect()
            if torch.backends.mps.is_available(): torch.mps.empty_cache()
            elif torch.cuda.is_available(): torch.cuda.empty_cache()
            print_memory_usage(f"AFTER_UNLOAD_{expert.name.upper()}")